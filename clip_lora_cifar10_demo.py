#!/usr/bin/env python3
"""
CLIP + LoRA è®­ç»ƒ CIFAR-10 æ•°æ®é›† Demo

è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨é¡¹ç›®ä¸­çš„CLIPæ¨¡å‹å’ŒLoRAæ¨¡å—æ¥è®­ç»ƒCIFAR-10æ•°æ®é›†ã€‚
é…ç½®ï¼š
- æ•°æ®é›†ï¼šCIFAR-10ï¼ˆ25000ä¸ªæ ·æœ¬ï¼‰
- LoRAå‚æ•°ï¼šr=8, alpha=16
- æ‰¹å¤§å°ï¼š32
- åŒ…å«è®­ç»ƒè¿›åº¦æ¡
"""

import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Subset
from torchvision import transforms
from tqdm import tqdm
import time
from typing import Dict, Any, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from models.clip import ImageEncoder
from lora.loRA_wrapper import LoRAWrapper
from lora.lora_config import LoRAConfig
from data.datasets.cifar10 import CIFAR10


class CLIPClassifier(nn.Module):
    """CLIPå›¾åƒç¼–ç å™¨ + åˆ†ç±»å¤´"""
    
    def __init__(self, model_name: str = "openai/clip-vit-base-patch32", num_classes: int = 10, cache_dir: str = None):
        super().__init__()
        self.image_encoder = ImageEncoder(model_name=model_name, cache_dir=cache_dir)
        # è·å–CLIPå›¾åƒç‰¹å¾ç»´åº¦ï¼ˆä»vision_modelçš„hidden_sizeè·å–ï¼‰
        feature_dim = self.image_encoder.vision_model.config.hidden_size
        
        # æ·»åŠ åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Dropout(0.1),
            nn.Linear(feature_dim, num_classes)
        )
        
    def forward(self, pixel_values):
        # è·å–å›¾åƒç‰¹å¾
        image_features = self.image_encoder(pixel_values)
        # åˆ†ç±»
        logits = self.classifier(image_features)
        return logits


def create_cifar10_dataloader(data_root: str = "./data", 
                             batch_size: int = 32, 
                             max_samples: int = 25000,
                             train: bool = True) -> DataLoader:
    """åˆ›å»ºCIFAR-10æ•°æ®åŠ è½½å™¨"""
    
    # CLIPæ¨¡å‹çš„é¢„å¤„ç†
    transform = transforms.Compose([
        transforms.Resize((224, 224)),  # CLIPéœ€è¦224x224è¾“å…¥
        transforms.ToTensor(),
        transforms.Normalize(
            mean=[0.48145466, 0.4578275, 0.40821073],  # CLIPé¢„è®­ç»ƒçš„æ ‡å‡†åŒ–å‚æ•°
            std=[0.26862954, 0.26130258, 0.27577711]
        )
    ])
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = CIFAR10(data_root=data_root, train=train, preprocess=transform)
    
    # å¦‚æœéœ€è¦é™åˆ¶æ ·æœ¬æ•°é‡ï¼Œåˆ›å»ºå­é›†
    if max_samples < len(dataset):
        indices = list(range(max_samples))
        dataset = Subset(dataset, indices)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=train,
        num_workers=2,
        pin_memory=True
    )
    
    return dataloader


def apply_lora_to_model(model: CLIPClassifier, r: int = 8, alpha: int = 16) -> LoRAWrapper:
    """ä¸ºCLIPæ¨¡å‹åº”ç”¨LoRA"""
    
    print(f"æ­£åœ¨åº”ç”¨LoRAé…ç½®: r={r}, alpha={alpha}")
    
    # åˆ›å»ºLoRAåŒ…è£…å™¨
    lora_wrapper = LoRAWrapper(model.image_encoder.vision_model)
    
    # åº”ç”¨LoRAé…ç½®
    lora_wrapper.apply_lora(
        r=r,
        lora_alpha=alpha,
        lora_dropout=0.1,
        target_modules=["q_proj", "v_proj", "k_proj", "out_proj", "fc1", "fc2"]
    )
    
    return lora_wrapper


def train_epoch(model: CLIPClassifier, 
                dataloader: DataLoader, 
                optimizer: optim.Optimizer, 
                criterion: nn.Module, 
                device: torch.device,
                epoch: int) -> Tuple[float, float]:
    """è®­ç»ƒä¸€ä¸ªepoch"""
    
    model.train()
    total_loss = 0.0
    correct = 0
    total = 0
    
    # åˆ›å»ºè¿›åº¦æ¡
    pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}", leave=False)
    
    for batch_idx, (images, labels) in enumerate(pbar):
        images, labels = images.to(device), labels.to(device)
        
        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        
        # åå‘ä¼ æ’­
        loss.backward()
        optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()
        
        # æ›´æ–°è¿›åº¦æ¡
        current_acc = 100. * correct / total
        current_loss = total_loss / (batch_idx + 1)
        pbar.set_postfix({
            'Loss': f'{current_loss:.4f}',
            'Acc': f'{current_acc:.2f}%',
            'LR': f'{optimizer.param_groups[0]["lr"]:.6f}'
        })
    
    epoch_loss = total_loss / len(dataloader)
    epoch_acc = 100. * correct / total
    
    return epoch_loss, epoch_acc


def evaluate(model: CLIPClassifier, 
             dataloader: DataLoader, 
             criterion: nn.Module, 
             device: torch.device) -> Tuple[float, float]:
    """è¯„ä¼°æ¨¡å‹"""
    
    model.eval()
    total_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        pbar = tqdm(dataloader, desc="Evaluating", leave=False)
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)
            
            outputs = model(images)
            loss = criterion(outputs, labels)
            
            total_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # æ›´æ–°è¿›åº¦æ¡
            current_acc = 100. * correct / total
            pbar.set_postfix({'Acc': f'{current_acc:.2f}%'})
    
    avg_loss = total_loss / len(dataloader)
    accuracy = 100. * correct / total
    
    return avg_loss, accuracy


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    
    print("ğŸš€ CLIP + LoRA CIFAR-10 è®­ç»ƒ Demo")
    print("=" * 50)
    
    # é…ç½®å‚æ•°
    config = {
        'batch_size': 32,
        'max_samples': 25000,
        'lora_r': 8,
        'lora_alpha': 16,
        'learning_rate': 2e-5,  # é™ä½å­¦ä¹ ç‡
        'epochs': 5,
        'data_root': '/home/zzm/dataset',  # CIFAR-10æ•°æ®é›†è·¯å¾„
        'model_name': "openai/clip-vit-base-patch32",
        'cache_dir': '/home/zzm/checkpoint'  # CLIPæ¨¡å‹ç¼“å­˜è·¯å¾„
    }
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs(config['data_root'], exist_ok=True)
    os.makedirs(config['cache_dir'], exist_ok=True)
    
    print("é…ç½®å‚æ•°:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print()
    
    print(f"æ•°æ®é›†è·¯å¾„: {config['data_root']}")
    print(f"æ¨¡å‹ç¼“å­˜è·¯å¾„: {config['cache_dir']}")
    print()
    
    # è®¾å¤‡é…ç½®
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    print()
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("ğŸ”„ æ­£åœ¨å‡†å¤‡æ•°æ®...")
    train_loader = create_cifar10_dataloader(
        data_root=config['data_root'],
        batch_size=config['batch_size'],
        max_samples=config['max_samples'],
        train=True
    )
    
    test_loader = create_cifar10_dataloader(
        data_root=config['data_root'],
        batch_size=config['batch_size'],
        max_samples=5000,  # æµ‹è¯•é›†ä½¿ç”¨5000ä¸ªæ ·æœ¬
        train=False
    )
    
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_loader.dataset)}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_loader.dataset)}")
    print(f"æ‰¹å¤§å°: {config['batch_size']}")
    print(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print()
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–æ¨¡å‹...")
    model = CLIPClassifier(
        model_name=config['model_name'], 
        num_classes=10,
        cache_dir=config['cache_dir']
    ).to(device)
    
    # åº”ç”¨LoRA
    lora_wrapper = apply_lora_to_model(
        model, 
        r=config['lora_r'], 
        alpha=config['lora_alpha']
    )
    print()
    
    # è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    # åªè®­ç»ƒLoRAå‚æ•°å’Œåˆ†ç±»å¤´
    lora_params = [p for p in model.image_encoder.vision_model.parameters() if p.requires_grad]
    classifier_params = list(model.classifier.parameters())
    trainable_params = lora_params + classifier_params
    
    optimizer = optim.AdamW(trainable_params, lr=config['learning_rate'], weight_decay=0.01)
    criterion = nn.CrossEntropyLoss()
    
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in trainable_params):,}")
    print(f"æ€»å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print()
    
    # è®­ç»ƒå¾ªç¯
    print("ğŸ¯ å¼€å§‹è®­ç»ƒ...")
    best_acc = 0.0
    
    for epoch in range(config['epochs']):
        print(f"\nğŸ“ˆ Epoch {epoch+1}/{config['epochs']}")
        
        # è®­ç»ƒ
        start_time = time.time()
        train_loss, train_acc = train_epoch(
            model, train_loader, optimizer, criterion, device, epoch
        )
        train_time = time.time() - start_time
        
        # è¯„ä¼°
        start_time = time.time()
        test_loss, test_acc = evaluate(model, test_loader, criterion, device)
        eval_time = time.time() - start_time
        
        # æ‰“å°ç»“æœ
        print(f"è®­ç»ƒ - Loss: {train_loss:.4f}, Acc: {train_acc:.2f}% ({train_time:.1f}s)")
        print(f"æµ‹è¯• - Loss: {test_loss:.4f}, Acc: {test_acc:.2f}% ({eval_time:.1f}s)")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if test_acc > best_acc:
            best_acc = test_acc
            print(f"ğŸ‰ æ–°çš„æœ€ä½³å‡†ç¡®ç‡: {best_acc:.2f}%")
            
            # ä¿å­˜LoRAæƒé‡
            torch.save({
                'lora_state_dict': {name: param for name, param in model.named_parameters() if param.requires_grad},
                'classifier_state_dict': model.classifier.state_dict(),
                'config': config,
                'best_acc': best_acc
            }, 'best_clip_lora_cifar10.pth')
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_acc:.2f}%")
    print("æ¨¡å‹å·²ä¿å­˜åˆ°: best_clip_lora_cifar10.pth")


if __name__ == "__main__":
    main()