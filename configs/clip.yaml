# CLIP模型联邦学习配置文件
# 专门用于CLIP多模态模型的联邦学习实验

# ==============================
# 实验基本设置
# ==============================
experiment:
  name: "clip_federated_learning"          # 实验名称
  rounds: 20                               # CLIP需要更多轮数收敛
  seed: 42                                 # 随机种子

# ==============================
# 设备配置
# ==============================
device: "auto"  # auto: 自动检测GPU, cuda: 强制GPU, cpu: 强制CPU

# ==============================
# 分布式训练和日志配置
# ==============================
wandb:
  enabled: true                            # 启用WandB，记录CLIP训练过程
  project: "clip-federated-learning"       # 专门的CLIP项目
  offline: true                            # 离线模式

# ==============================
# 客户端配置
# ==============================
client:
  num_clients: 4                           # 客户端数量
  local_epochs: 2                          # CLIP训练使用较少的本地轮数
  learning_rate: 0.00005                   # CLIP使用较小的学习率
  
  # 客户端数据集分配 - 支持多模态数据
  client_datasets:
    client_0:
      - mnist                              # 单模态数据
    client_1:
      - mnist
    client_2:
      - cifar10                            # 不同的视觉数据
    client_3:
      - mnist                              # 混合数据集
      - cifar10

# ==============================
# 服务器配置
# ==============================
server:
  aggregation_method: "federated_avg"      # FedAvg算法
  server_learning_rate: 1.0                # 服务器学习率

# ==============================
# 模型配置
# ==============================
model:
  type: "clip"                             # CLIP模型类型
  learning_rate: 0.00005                   # CLIP模型学习率
  
  # Hugging Face模型配置
  model_name: "openai/clip-vit-base-patch32"  # 预训练模型名称
  cache_dir: "/home/zzm/checkpoint"        # 模型缓存目录
  use_pretrained: true                     # 使用预训练权重
  
  # CLIP特有参数
  temperature: 0.07                        # 对比学习温度参数
  freeze_vision_encoder: false             # 是否冻结视觉编码器
  freeze_text_encoder: false               # 是否冻结文本编码器
  
  # LoRA微调配置（默认关闭）
  lora:
    enabled: false                         # 是否启用LoRA微调
    r: 16                                  # LoRA rank
    lora_alpha: 32                         # LoRA scaling参数
    lora_dropout: 0.1                      # LoRA dropout概率
    target_modules: ["q_proj", "v_proj", "k_proj", "out_proj"]
  
  # 图像编码器配置
  image_encoder:
    feature_dim: 768                       # ViT-B/32的特征维度
  
  # 文本编码器配置
  text_encoder:
    feature_dim: 512                       # 文本特征维度
    max_length: 77                         # 最大文本长度

# ==============================
# 优化器配置
# ==============================
optimizer:
  type: "adamw"                            # 优化器类型
  learning_rate: 5e-5                      # CLIP专用学习率
  weight_decay: 0.1                        # CLIP使用较大的权重衰减
  betas: [0.9, 0.98]                      # CLIP优化的beta参数
  eps: 1e-8                               # 数值稳定性参数

# ==============================
# 数据集配置
# ==============================
data:
  data_dir: "/home/zzm/dataset/"           # 数据集根目录
  batch_size: 32                           # 较小批次，适合CLIP大模型
  num_workers: 2                           # 数据加载器工作进程数
  
  # 具体数据集配置
  datasets:
    mnist:
      train: true                          # MNIST数据集
    cifar10:
      train: true                          # CIFAR-10数据集
    # 注释掉的CLIP专用数据集配置，暂时不使用
    # clip:
    #   train: true
    #   image_size: 224
    #   normalize: true
    #   transforms:
    #     resize: 224
    #     center_crop: 224
    #     normalize:
    #       mean: [0.48145466, 0.4578275, 0.40821073]
    #       std: [0.26862954, 0.26130258, 0.27577711]

# ==============================
# 评估配置
# ==============================
evaluation:
  evaluate_every: 1                        # 每轮都评估
  test_batch_size: 64                      # 测试批次大小
  metrics:                                 # 评估指标
    - "loss"
    - "accuracy"
    - "contrastive_loss"                   # CLIP特有的对比损失
    - "accuracy"
    - "top5_accuracy"
