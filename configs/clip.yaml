# CLIP模型联邦学习配置文件

# 实验基本设置
experiment:
  name: "clip_federated_learning"
  rounds: 20
  seed: 42

# 设备配置
device: "auto"  # auto: 自动检测GPU, cuda: 强制GPU, cpu: 强制CPU

# WandB配置
wandb:
  enabled: false
  project: "clip-federated-learning"
  offline: false

# 客户端设置
client:
  num_clients: 5
  local_epochs: 2
  learning_rate: 0.00005
  
  # 客户端数据集配置 - 所有客户端使用CLIP数据集
  client_datasets:
    client_0:
      - clip
    client_1:
      - clip
    client_2:
      - clip
    client_3:
      - clip
    client_4:
      - clip

# 服务器设置
server:
  aggregation_method: "federated_avg"
  server_learning_rate: 1.0

# 模型设置
model:
  type: "clip"
  learning_rate: 0.00005
  
  # Hugging Face模型配置
  model_name: "openai/clip-vit-base-patch32"
  cache_dir: "./models/clip_cache"
  use_pretrained: true
  
  # CLIP特有参数
  temperature: 0.07
  freeze_vision_encoder: false
  freeze_text_encoder: false
  
  # 图像编码器配置
  image_encoder:
    feature_dim: 768  # ViT-B/32的特征维度
  
  # 文本编码器配置
  text_encoder:
    feature_dim: 512
    max_length: 77

# 优化器设置 - 统一使用AdamW
optimizer:
  learning_rate: 5e-5
  weight_decay: 0.1
  betas: [0.9, 0.98]
  eps: 0.00000001

# 数据设置
data:
  data_dir: "/home/zzm/dataset/"
  batch_size: 16
  num_workers: 2
  
  # 数据集配置
  datasets:
    clip:
      train: true
      image_size: 224
      normalize: true
      # 数据变换配置
      transforms:
        resize: 224
        center_crop: 224
        normalize:
          mean: [0.48145466, 0.4578275, 0.40821073]
          std: [0.26862954, 0.26130258, 0.27577711]

# 评估设置
evaluation:
  evaluate_every: 5
  test_batch_size: 32
  metrics: 
    - "loss"
    - "accuracy"
    - "top5_accuracy"
