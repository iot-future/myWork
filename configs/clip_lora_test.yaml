# CLIP模型LoRA微调配置
# 启用LoRA微调功能的简化配置

experiment:
  name: "clip_lora_federated_learning"
  rounds: 15
  seed: 42

device: "auto"

# ==============================
# 学习率配置 - 统一管理所有学习率设置
# ==============================
learning_rates:
  client: 0.00005                          # CLIP LoRA客户端学习率
  server: 1.0                              # 服务器聚合学习率
  model: 0.00005                           # CLIP LoRA模型训练学习率
  optimizer: 5e-5                          # CLIP LoRA优化器学习率

wandb:
  enabled: true
  project: "clip-lora-federated-learning"
  offline: true

client:
  num_clients: 3
  local_epochs: 2
  
  client_datasets:
    client_0:
      - cifar10                              # 客户端0：仅MNIST
    client_1:
      - mnist                              # 客户端1：仅MNIST
    client_2:
      - cifar10    

server:
  aggregation_method: "federated_avg"

model:
  type: "clip"
  model_name: "openai/clip-vit-base-patch32"
  cache_dir: "/home/zzm/checkpoint"
  use_pretrained: true
  
  # LoRA配置
  lora:
    enabled: true
    r: 8
    lora_alpha: 16
    lora_dropout: 0.05
    target_modules: ["q_proj", "v_proj", "k_proj", "out_proj"]

optimizer:
  type: "adamw"
  weight_decay: 0.01
  betas: [0.9, 0.98]      

data:
  data_dir: "/home/zzm/dataset/"
  batch_size: 32
  num_workers: 2
  
  datasets:
    mnist:
    cifar10:
    cifar100:

evaluation:
  evaluate_every: 1
  test_batch_size: 64
  metrics: ["loss", "accuracy", "top5_accuracy"]