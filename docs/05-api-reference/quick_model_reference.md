# æ–°å¢æ¨¡å‹å¿«é€Ÿå‚è€ƒæŒ‡å—

è¿™æ˜¯ä¸€ä¸ªç²¾ç®€ç‰ˆçš„æ¨¡å‹æ·»åŠ æŒ‡å—ï¼Œé€‚åˆæœ‰ç»éªŒçš„å¼€å‘è€…å¿«é€Ÿå‚è€ƒã€‚

## ğŸš€ å¿«é€Ÿæ­¥éª¤

### 1. åˆ›å»ºæ¨¡å‹æ–‡ä»¶
```python
# models/your_model.py
from core.base import BaseModel

class YourModel(BaseModel):
    def __init__(self, optimizer_config=None):
        super().__init__(optimizer_config)
        self.model = nn.Sequential(...)  # å®šä¹‰æ¶æ„
        self.create_optimizer(self.model.parameters())
        self.criterion = nn.CrossEntropyLoss()
    
    def get_parameters(self): 
        return {name: param.data.clone() for name, param in self.model.named_parameters()}
    
    def set_parameters(self, params):
        with torch.no_grad():
            for name, param in self.model.named_parameters():
                if name in params:
                    param.data.copy_(params[name])
    
    def train_step(self, data, labels):
        # å®ç°è®­ç»ƒé€»è¾‘
        pass
    
    def evaluate(self, data, labels):
        # å®ç°è¯„ä¼°é€»è¾‘  
        pass
```

### 2. æ›´æ–°æ¨¡å‹å·¥å‚
```python
# utils/model_factory.py
from models.your_model import YourModel  # æ·»åŠ å¯¼å…¥

# åœ¨ create_model æ–¹æ³•ä¸­æ·»åŠ ï¼š
elif model_type == 'your_model':
    return YourModel(optimizer_config=optimizer_config)

# åœ¨ get_supported_models ä¸­æ·»åŠ ï¼š
return [..., 'your_model']
```

### 3. æ›´æ–°å¯¼å‡º
```python
# models/__init__.py
from .your_model import YourModel
__all__ = [..., 'YourModel']
```

### 4. åˆ›å»ºé…ç½®æ–‡ä»¶
```yaml
# configs/your_model.yaml
experiment:
  name: "your_model_experiment"
  rounds: 50

model:
  type: "your_model"
  # æ·»åŠ æ¨¡å‹å‚æ•°

optimizer:
  type: "adam"
  learning_rate: 0.001
```

### 5. æµ‹è¯•éªŒè¯
```python
# å¿«é€Ÿæµ‹è¯•
from utils.model_factory import ModelFactory
model = ModelFactory.create_model({'type': 'your_model'})
print("âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
```

## âœ… æ£€æŸ¥æ¸…å•

- [ ] ç»§æ‰¿äº† `BaseModel`
- [ ] å®ç°äº†æ‰€æœ‰æŠ½è±¡æ–¹æ³•ï¼ˆ4ä¸ªï¼‰
- [ ] åœ¨æ¨¡å‹å·¥å‚ä¸­æ·»åŠ äº†åˆ›å»ºé€»è¾‘
- [ ] æ›´æ–°äº† `__init__.py` å¯¼å‡º
- [ ] åˆ›å»ºäº†é…ç½®æ–‡ä»¶
- [ ] åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡

## ğŸ¯ å¿…é¡»å®ç°çš„æ–¹æ³•

| æ–¹æ³•å | ç”¨é€” | è¿”å›å€¼ |
|--------|------|--------|
| `get_parameters()` | è”é‚¦å­¦ä¹ å‚æ•°è·å– | Dict[str, Tensor] |
| `set_parameters()` | è”é‚¦å­¦ä¹ å‚æ•°è®¾ç½® | None |
| `train_step()` | å•æ­¥è®­ç»ƒ | float (loss) |
| `evaluate()` | æ¨¡å‹è¯„ä¼° | Dict[str, float] |

## âš ï¸ å¸¸è§é™·é˜±

1. **å¿˜è®°è°ƒç”¨ `super().__init__()`**
2. **æ²¡æœ‰æä¾›é»˜è®¤ä¼˜åŒ–å™¨**
3. **å‚æ•°åä¸åŒ¹é…å¯¼è‡´ set_parameters å¤±è´¥**
4. **å¿˜è®°åœ¨å·¥å‚ç±»ä¸­æ·»åŠ æ¨¡å‹ç±»å‹**

## ğŸ“ æ¨¡å‹æ¨¡æ¿

å¤åˆ¶ä»¥ä¸‹æ¨¡æ¿å¿«é€Ÿå¼€å§‹ï¼š

```python
# models/template.py
import torch
import torch.nn as nn
from core.base import BaseModel

class TemplateModel(BaseModel):
    def __init__(self, optimizer_config=None):
        super().__init__(optimizer_config)
        
        # å®šä¹‰æ¨¡å‹
        self.model = nn.Sequential(
            nn.Linear(784, 128),
            nn.ReLU(),
            nn.Linear(128, 10)
        )
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.create_optimizer(self.model.parameters())
        if self.optimizer is None:
            self.optimizer = torch.optim.Adam(self.model.parameters())
        
        self.criterion = nn.CrossEntropyLoss()
    
    def get_parameters(self):
        return {n: p.data.clone() for n, p in self.model.named_parameters()}
    
    def set_parameters(self, params):
        with torch.no_grad():
            for n, p in self.model.named_parameters():
                if n in params: p.data.copy_(params[n])
    
    def train_step(self, data, labels):
        self.model.train()
        self.optimizer.zero_grad()
        outputs = self.model(data)
        loss = self.criterion(outputs, labels)
        loss.backward()
        self.optimizer.step()
        return loss.item()
    
    def evaluate(self, data, labels):
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(data)
            loss = self.criterion(outputs, labels)
            _, pred = torch.max(outputs, 1)
            acc = (pred == labels).float().mean().item()
        return {'loss': loss.item(), 'accuracy': acc}
```

åªéœ€ä¿®æ”¹æ¨¡å‹æ¶æ„éƒ¨åˆ†å³å¯ï¼
