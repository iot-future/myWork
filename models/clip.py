"""
CLIP (Contrastive Language-Image Pre-training) æ¨¡å‹å®ç°
åŸºäºHugging Face transformersåº“çš„å®ç°ï¼Œæ”¯æŒè”é‚¦å­¦ä¹ æ¡†æ¶
è§£è€¦æ¶æ„è®¾è®¡ï¼Œåˆ†ç¦»å›¾åƒç¼–ç å™¨ã€æ–‡æœ¬ç¼–ç å™¨å’Œåˆ†ç±»å¤´

å‚è€ƒè®ºæ–‡ï¼š
Learning Transferable Visual Representations with Natural Language Supervision
Radford et al., 2021
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, Optional, Union, List, Tuple
from transformers import CLIPModel, CLIPProcessor, CLIPVisionModel, CLIPTextModel
from transformers import AutoProcessor, AutoModel
from PIL import Image
from core.base import BaseModel
from utils.device_manager import device_manager

# LoRAç›¸å…³å¯¼å…¥
try:
    from lora.clip_lora import CLIPLoRAWrapper
    LORA_AVAILABLE = True
except ImportError:
    LORA_AVAILABLE = False
    print("Warning: LoRA functionality not available. Please install required dependencies.")


class ImageEncoder(torch.nn.Module):
    """
    å›¾åƒç¼–ç å™¨ç±»
    åŸºäºHugging Face CLIPæ¨¡å‹çš„å›¾åƒç¼–ç å™¨ï¼Œç”¨äºå°†å›¾åƒè½¬æ¢ä¸ºç‰¹å¾å‘é‡
    """
    def __init__(self, model_name: str = "openai/clip-vit-base-patch32", 
                 cache_dir: Optional[str] = None, device: Optional[str] = None):
        """
        åˆå§‹åŒ–å›¾åƒç¼–ç å™¨
        
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º"openai/clip-vit-base-patch32"
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
            device: è®¾å¤‡ç±»å‹
        """
        super().__init__()
        
        print(f'Loading {model_name} pre-trained weights.')
        
        # ä½¿ç”¨Hugging Faceçš„CLIPè§†è§‰æ¨¡å‹
        self.vision_model = CLIPVisionModel.from_pretrained(
            model_name, 
            cache_dir=cache_dir
        )
        
        # åˆ›å»ºå¤„ç†å™¨ç”¨äºå›¾åƒé¢„å¤„ç†
        self.processor = CLIPProcessor.from_pretrained(
            model_name,
            cache_dir=cache_dir
        )
        
        self.cache_dir = cache_dir
        self.model_name = model_name
        
        # è·å–ç‰¹å¾ç»´åº¦
        self.feature_dim = self.vision_model.config.hidden_size
        
        # è®¾å¤‡ç¼“å­˜ä¼˜åŒ–
        self._device_cache = None
        self._device_cache_dirty = True
        
        # è®¾ç½®è®¾å¤‡
        if device:
            device_manager.move_model_to_device(self, torch.device(device))

    def _get_device(self):
        """è·å–æ¨¡å‹è®¾å¤‡ - å¸¦ç¼“å­˜ä¼˜åŒ–"""
        if self._device_cache is None or self._device_cache_dirty:
            try:
                self._device_cache = next(self.vision_model.parameters()).device
                self._device_cache_dirty = False
            except StopIteration:
                self._device_cache = torch.device('cpu')
        return self._device_cache

    def to(self, device):
        """ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡å¹¶æ ‡è®°ç¼“å­˜å¤±æ•ˆ"""
        result = super().to(device)
        self._device_cache_dirty = True
        return result

    def forward(self, images):
        """
        å‰å‘ä¼ æ’­ï¼Œå°†å›¾åƒç¼–ç ä¸ºç‰¹å¾å‘é‡
        
        Args:
            images: è¾“å…¥çš„å›¾åƒå¼ é‡æˆ–PILå›¾åƒåˆ—è¡¨
            
        Returns:
            ç¼–ç åçš„å›¾åƒç‰¹å¾å‘é‡
        """
        # è·å–è®¾å¤‡ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰
        device = self._get_device()
        
        # å¦‚æœè¾“å…¥æ˜¯PILå›¾åƒåˆ—è¡¨ï¼Œå…ˆè¿›è¡Œé¢„å¤„ç†
        if isinstance(images, list) and isinstance(images[0], Image.Image):
            inputs = self.processor(images=images, return_tensors="pt", padding=True)
            pixel_values = device_manager.move_tensors_to_device(inputs['pixel_values'], device=device)
        elif isinstance(images, torch.Tensor):
            pixel_values = device_manager.move_tensors_to_device(images, device=device)
        else:
            raise ValueError("Images must be either a list of PIL Images or a torch.Tensor")
            
        # é€šè¿‡è§†è§‰ç¼–ç å™¨è·å–ç‰¹å¾
        vision_outputs = self.vision_model(pixel_values=pixel_values)
        # è¿”å›pooledè¾“å‡ºï¼ˆCLS tokençš„è¡¨ç¤ºï¼‰
        return vision_outputs.pooler_output
    

    def save(self, filename: str):
        """
        ä¿å­˜å›¾åƒç¼–ç å™¨åˆ°checkpointæ–‡ä»¶
        
        Args:
            filename: ä¿å­˜æ–‡ä»¶çš„è·¯å¾„
        """
        print(f'Saving image encoder to {filename}')
        torch.save({
            'model_state_dict': self.vision_model.state_dict(),
            'model_name': self.model_name,
            'cache_dir': self.cache_dir
        }, filename)

    @classmethod
    def load(cls, filename: str):
        """
        ä»checkpointåŠ è½½å›¾åƒç¼–ç å™¨
        
        Args:
            filename: åŠ è½½checkpoinæ–‡ä»¶çš„è·¯å¾„
            
        Returns:
            åŠ è½½çš„å›¾åƒç¼–ç å™¨å®ä¾‹
        """
        print(f'Loading image encoder from {filename}')
        checkpoint = torch.load(filename, map_location='cpu')
        
        encoder = cls(
            model_name=checkpoint['model_name'],
            cache_dir=checkpoint['cache_dir']
        )
        encoder.vision_model.load_state_dict(checkpoint['model_state_dict'])
        return encoder


class TextEncoder(torch.nn.Module):
    """
    æ–‡æœ¬ç¼–ç å™¨ç±»
    åŸºäºHugging Face CLIPæ¨¡å‹çš„æ–‡æœ¬ç¼–ç å™¨ï¼Œç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºç‰¹å¾å‘é‡
    """
    def __init__(self, model_name: str = "openai/clip-vit-base-patch32",
                 cache_dir: Optional[str] = None, device: Optional[str] = None):
        """
        åˆå§‹åŒ–æ–‡æœ¬ç¼–ç å™¨
        
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
            device: è®¾å¤‡ç±»å‹
        """
        super().__init__()
        
        print(f'Loading {model_name} text encoder pre-trained weights.')
        
        # ä½¿ç”¨Hugging Faceçš„CLIPæ–‡æœ¬æ¨¡å‹
        self.text_model = CLIPTextModel.from_pretrained(
            model_name,
            cache_dir=cache_dir
        )
        
        # åˆ›å»ºå¤„ç†å™¨ç”¨äºæ–‡æœ¬é¢„å¤„ç†
        self.processor = CLIPProcessor.from_pretrained(
            model_name,
            cache_dir=cache_dir
        )
        
        self.cache_dir = cache_dir
        self.model_name = model_name
        
        # è·å–ç‰¹å¾ç»´åº¦
        self.feature_dim = self.text_model.config.hidden_size
        
        # è®¾å¤‡ç¼“å­˜ä¼˜åŒ–
        self._device_cache = None
        self._device_cache_dirty = True
        
        if device:
            device_manager.move_model_to_device(self, torch.device(device))

    def _get_device(self):
        """è·å–æ¨¡å‹è®¾å¤‡ - å¸¦ç¼“å­˜ä¼˜åŒ–"""
        if self._device_cache is None or self._device_cache_dirty:
            try:
                self._device_cache = next(self.text_model.parameters()).device
                self._device_cache_dirty = False
            except StopIteration:
                self._device_cache = torch.device('cpu')
        return self._device_cache

    def to(self, device):
        """ç§»åŠ¨æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡å¹¶æ ‡è®°ç¼“å­˜å¤±æ•ˆ"""
        result = super().to(device)
        self._device_cache_dirty = True
        return result

    def forward(self, texts: Union[List[str], torch.Tensor]):
        """
        å‰å‘ä¼ æ’­ï¼Œå°†æ–‡æœ¬ç¼–ç ä¸ºç‰¹å¾å‘é‡
        
        Args:
            texts: è¾“å…¥çš„æ–‡æœ¬åˆ—è¡¨æˆ–tokenå¼ é‡
            
        Returns:
            ç¼–ç åçš„æ–‡æœ¬ç‰¹å¾å‘é‡
        """
        # è·å–è®¾å¤‡ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰
        device = self._get_device()
        
        if isinstance(texts, list):
            # æ–‡æœ¬é¢„å¤„ç†
            inputs = self.processor(text=texts, return_tensors="pt", padding=True, truncation=True)
            input_ids, attention_mask = device_manager.move_tensors_to_device(
                inputs['input_ids'], inputs['attention_mask'], device=device
            )
        elif isinstance(texts, torch.Tensor):
            input_ids = texts
            attention_mask = None
        else:
            raise ValueError("Texts must be either a list of strings or a torch.Tensor")
            
        # é€šè¿‡æ–‡æœ¬ç¼–ç å™¨è·å–ç‰¹å¾
        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)
        # è¿”å›pooledè¾“å‡º
        return text_outputs.pooler_output
    

    def save(self, filename: str):
        """ä¿å­˜æ–‡æœ¬ç¼–ç å™¨åˆ°checkpointæ–‡ä»¶"""
        print(f'Saving text encoder to {filename}')
        torch.save({
            'model_state_dict': self.text_model.state_dict(),
            'model_name': self.model_name,
            'cache_dir': self.cache_dir
        }, filename)

    @classmethod
    def load(cls, filename: str):
        """åŠ è½½ä»checkpointæ–‡ä»¶çš„æ–‡æœ¬ç¼–ç å™¨"""
        print(f'Loading text encoder from {filename}')
        checkpoint = torch.load(filename, map_location='cpu')
        
        encoder = cls(
            model_name=checkpoint['model_name'],
            cache_dir=checkpoint['cache_dir']
        )
        encoder.text_model.load_state_dict(checkpoint['model_state_dict'])
        return encoder


class ClassificationHead(torch.nn.Linear):
    """
    åˆ†ç±»å¤´ç±»
    ç»§æ‰¿è‡ªtorch.nn.Linearï¼Œç”¨äºå°†ç‰¹å¾å‘é‡æ˜ å°„åˆ°ç±»åˆ«æ¦‚ç‡
    æ”¯æŒç‰¹å¾å½’ä¸€åŒ–åŠŸèƒ½
    """
    def __init__(self, input_size: int, output_size: int, normalize: bool = False, 
                 bias: bool = True):
        """
        åˆå§‹åŒ–åˆ†ç±»å¤´
        
        Args:
            input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
            output_size: è¾“å‡ºç±»åˆ«æ•°
            normalize: æ˜¯å¦å¯¹è¾“å…¥ç‰¹å¾è¿›è¡ŒL2å½’ä¸€åŒ–
            bias: æ˜¯å¦ä½¿ç”¨åç½®
        """
        super().__init__(input_size, output_size, bias=bias)
        self.normalize = normalize
        
        # åˆå§‹åŒ–æƒé‡
        nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            nn.init.zeros_(self.bias)

    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        
        Args:
            inputs: è¾“å…¥ç‰¹å¾å‘é‡
            
        Returns:
            åˆ†ç±»logits
        """
        # å¦‚æœéœ€è¦å½’ä¸€åŒ–ï¼Œå¯¹è¾“å…¥è¿›è¡ŒL2å½’ä¸€åŒ–
        if self.normalize:
            inputs = F.normalize(inputs, dim=-1, p=2)
        return super().forward(inputs)

    def __call__(self, inputs):
        """ä½¿å¯¹è±¡å¯è°ƒç”¨"""
        return self.forward(inputs)

    def save(self, filename: str):
        """ä¿å­˜åˆ†ç±»å¤´"""
        print(f'Saving classification head to {filename}')
        torch.save({
            'state_dict': self.state_dict(),
            'input_size': self.in_features,
            'output_size': self.out_features,
            'normalize': self.normalize,
            'bias': self.bias is not None
        }, filename)

    @classmethod
    def load(cls, filename: str):
        """åŠ è½½åˆ†ç±»å¤´"""
        print(f'Loading classification head from {filename}')
        checkpoint = torch.load(filename, map_location='cpu')
        
        head = cls(
            input_size=checkpoint['input_size'],
            output_size=checkpoint['output_size'],
            normalize=checkpoint['normalize'],
            bias=checkpoint['bias']
        )
        head.load_state_dict(checkpoint['state_dict'])
        return head


class ImageClassifier(torch.nn.Module):
    """
    å›¾åƒåˆ†ç±»å™¨ç±»
    ç»“åˆå›¾åƒç¼–ç å™¨å’Œåˆ†ç±»å¤´çš„å®Œæ•´å›¾åƒåˆ†ç±»æ¨¡å‹
    """
    def __init__(self, image_encoder: ImageEncoder, classification_head: ClassificationHead):
        """
        åˆå§‹åŒ–å›¾åƒåˆ†ç±»å™¨
        
        Args:
            image_encoder: å›¾åƒç¼–ç å™¨å®ä¾‹
            classification_head: åˆ†ç±»å¤´å®ä¾‹
        """
        super().__init__()
        self.image_encoder = image_encoder
        self.classification_head = classification_head

    def freeze_encoder(self):
        """å†»ç»“å›¾åƒç¼–ç å™¨çš„å‚æ•°ï¼Œä½¿å…¶åœ¨è®­ç»ƒæ—¶ä¸æ›´æ–°"""
        for param in self.image_encoder.parameters():
            param.requires_grad_(False)

    def unfreeze_encoder(self):
        """è§£å†»å›¾åƒç¼–ç å™¨çš„å‚æ•°"""
        for param in self.image_encoder.parameters():
            param.requires_grad_(True)

    def freeze_head(self):
        """å†»ç»“åˆ†ç±»å¤´çš„å‚æ•°ï¼Œä½¿å…¶åœ¨è®­ç»ƒæ—¶ä¸æ›´æ–°"""
        for param in self.classification_head.parameters():
            param.requires_grad_(False)

    def unfreeze_head(self):
        """è§£å†»åˆ†ç±»å¤´çš„å‚æ•°"""
        for param in self.classification_head.parameters():
            param.requires_grad_(True)

    def forward(self, inputs):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            inputs: è¾“å…¥å›¾åƒ
            
        Returns:
            åˆ†ç±»ç»“æœ
        """
        # é€šè¿‡å›¾åƒç¼–ç å™¨æå–ç‰¹å¾
        features = self.image_encoder(inputs)
        # é€šè¿‡åˆ†ç±»å¤´å¾—åˆ°åˆ†ç±»ç»“æœ
        outputs = self.classification_head(features)
        return outputs

    def __call__(self, inputs):
        """ä½¿å¯¹è±¡å¯è°ƒç”¨"""
        return self.forward(inputs)

    def save(self, filename: str):
        """ä¿å­˜å›¾åƒåˆ†ç±»å™¨"""
        print(f'Saving image classifier to {filename}')
        torch.save({
            'image_encoder': self.image_encoder.state_dict(),
            'classification_head': self.classification_head.state_dict(),
            'encoder_model_name': self.image_encoder.model_name,
            'head_config': {
                'input_size': self.classification_head.in_features,
                'output_size': self.classification_head.out_features,
                'normalize': self.classification_head.normalize,
                'bias': self.classification_head.bias is not None
            }
        }, filename)

    @classmethod
    def load(cls, filename: str):
        """åŠ è½½å›¾åƒåˆ†ç±»å™¨"""
        print(f'Loading image classifier from {filename}')
        checkpoint = torch.load(filename, map_location='cpu')
        
        # é‡å»ºå›¾åƒç¼–ç å™¨
        image_encoder = ImageEncoder(model_name=checkpoint['encoder_model_name'])
        image_encoder.load_state_dict(checkpoint['image_encoder'])
        
        # é‡å»ºåˆ†ç±»å¤´
        head_config = checkpoint['head_config']
        classification_head = ClassificationHead(
            input_size=head_config['input_size'],
            output_size=head_config['output_size'],
            normalize=head_config['normalize'],
            bias=head_config['bias']
        )
        classification_head.load_state_dict(checkpoint['classification_head'])
        
        return cls(image_encoder, classification_head)


class FederatedCLIPModel(BaseModel):
    """è”é‚¦å­¦ä¹ CLIPæ¨¡å‹åŒ…è£…å™¨
    
    è¿™æ˜¯ä¸€ä¸ªåŒ…è£…å™¨ç±»ï¼Œå°†CLIPå¤šæ¨¡æ€æ¨¡å‹é€‚é…åˆ°è”é‚¦å­¦ä¹ æ¡†æ¶ä¸­ã€‚
    æä¾›ç»Ÿä¸€çš„å‚æ•°ç®¡ç†ã€è®¾å¤‡å…¼å®¹ã€è®­ç»ƒæ¥å£ç­‰è”é‚¦å­¦ä¹ ç‰¹æ€§ã€‚
    
    å®Œæ•´çš„CLIPæ¨¡å‹å®ç°ï¼Œç»§æ‰¿è‡ªBaseModelï¼Œé€‚é…è”é‚¦å­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒå›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚
    """
    def __init__(self, 
                 model_name: str = "openai/clip-vit-base-patch32",
                 num_classes: int = 10,
                 normalize_features: bool = True,
                 freeze_encoder: bool = False,
                 cache_dir: Optional[str] = None,
                 optimizer_config: Optional[Dict[str, Any]] = None,
                 checkpoint_path: Optional[str] = None,
                 lora_config: Optional[Dict[str, Any]] = None):
        """
        åˆå§‹åŒ–è”é‚¦å­¦ä¹ CLIPæ¨¡å‹åŒ…è£…å™¨
        
        Args:
            model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
            num_classes: åˆ†ç±»ç±»åˆ«æ•°
            normalize_features: æ˜¯å¦å¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–
            freeze_encoder: æ˜¯å¦å†»ç»“ç¼–ç å™¨
            cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
            optimizer_config: ä¼˜åŒ–å™¨é…ç½®
            checkpoint_path: å¦‚æœæä¾›ï¼Œå°†ä»æ­¤è·¯å¾„åŠ è½½é¢„è®­ç»ƒæƒé‡
            lora_config: LoRAé…ç½®ï¼ŒåŒ…å«enabledã€rã€lora_alphaç­‰å‚æ•°
        """
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        super().__init__(optimizer_config)
        
        self.model_name = model_name
        self.num_classes = num_classes
        self.normalize_features = normalize_features
        self.cache_dir = cache_dir
        self.lora_config = lora_config or {}
        
        # åˆ›å»ºå›¾åƒç¼–ç å™¨
        self.image_encoder = ImageEncoder(
            model_name=self.model_name,
            cache_dir=self.cache_dir
        )
        
        # åˆ›å»ºåˆ†ç±»å¤´
        self.classification_head = ClassificationHead(
            input_size=self.image_encoder.feature_dim,
            output_size=self.num_classes,
            normalize=self.normalize_features
        )
        
        # ç»„åˆæˆå®Œæ•´çš„åˆ†ç±»å™¨
        self.classifier = ImageClassifier(self.image_encoder, self.classification_head)
        
        # åˆå§‹åŒ–LoRAåŒ…è£…å™¨
        self.lora_wrapper = None
        self._lora_enabled = False
        
        # è®¾å¤‡ç¼“å­˜ä¼˜åŒ–
        self._device_cache = None
        self._device_cache_dirty = True
        
        # åº”ç”¨LoRAï¼ˆå¦‚æœé…ç½®ä¸­å¯ç”¨ï¼‰
        if self.lora_config.get('enabled', False) and LORA_AVAILABLE:
            self._setup_lora()
        
        # å¦‚æœéœ€è¦å†»ç»“ç¼–ç å™¨
        if freeze_encoder:
            self.classifier.freeze_encoder()
        
        # åˆ›å»ºAdamWä¼˜åŒ–å™¨
        self.create_optimizer(self.classifier.parameters())
        if self.optimizer is None:
            # å›é€€åˆ°é»˜è®¤AdamWé…ç½®
            from utils.optimizer_factory import OptimizerFactory
            # CLIPä¸“ç”¨çš„é»˜è®¤é…ç½®
            default_config = {
                'learning_rate': 5e-5,
                'weight_decay': 0.1,
                'betas': [0.9, 0.98],
                'eps': 1e-6
            }
            self.optimizer = OptimizerFactory.create_optimizer(
                self.classifier.parameters(), default_config
            )
        
        # å¦‚æœæä¾›äº†checkpointè·¯å¾„ï¼ŒåŠ è½½é¢„è®­ç»ƒæƒé‡
        if checkpoint_path is not None:
            self.load_model(checkpoint_path)
        
        # å®šä¹‰æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss()
    
    def _setup_lora(self):
        """è®¾ç½®LoRAå¾®è°ƒ"""
        if not LORA_AVAILABLE:
            print("âš ï¸  è­¦å‘Š: LoRAåŠŸèƒ½ä¸å¯ç”¨ï¼Œè¯·å®‰è£…æ‰€éœ€ä¾èµ–")
            return
        
        try:
            # åˆ›å»ºLoRAåŒ…è£…å™¨
            self.lora_wrapper = CLIPLoRAWrapper(vision_model=self.image_encoder.vision_model)
            
            # ç®€åŒ–é…ç½®å¤„ç†
            vision_config = {
                'r': self.lora_config.get('r', 16),
                'lora_alpha': self.lora_config.get('lora_alpha', 32),
                'lora_dropout': self.lora_config.get('lora_dropout', 0.1),
                'target_modules': self.lora_config.get('target_modules', ["q_proj", "v_proj", "k_proj", "out_proj"])
            }
            
            # åº”ç”¨LoRA
            self.lora_wrapper.apply_lora(vision_config=vision_config)
            self._lora_enabled = True
            
            # è¾“å‡ºå…³é”®çš„LoRAç»Ÿè®¡ä¿¡æ¯
            trainable_params = self.lora_wrapper.get_trainable_parameters()
            total_original_params = sum(p.numel() for p in self.image_encoder.vision_model.parameters())
            
            print(f"ğŸ¯ LoRAè®¾ç½®å®Œæˆ | å‚æ•°æ•ˆç‡: {(trainable_params/total_original_params)*100:.2f}% ({trainable_params:,}/{total_original_params:,})")
                
        except Exception as e:
            print(f"âŒ LoRAè®¾ç½®å¤±è´¥: {e}")
            self.lora_wrapper = None
            self._lora_enabled = False
        
    def to(self, device):
        """å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        device_manager.move_model_to_device(self.classifier, device)
        device_manager.move_model_to_device(self.criterion, device)
        # è®¾å¤‡å‘ç”Ÿå˜åŒ–æ—¶ï¼Œæ ‡è®°ç¼“å­˜å¤±æ•ˆ
        self._device_cache_dirty = True
        return self
        
    def get_parameters(self) -> Dict[str, torch.Tensor]:
        """
        è·å–æ¨¡å‹å‚æ•° - è”é‚¦å­¦ä¹ æ ¸å¿ƒåŠŸèƒ½
        
        å½“å¯ç”¨LoRAæ—¶ï¼Œåªè¿”å›LoRAå‚æ•°å’Œåˆ†ç±»å¤´å‚æ•°
        å½“æœªå¯ç”¨LoRAæ—¶ï¼Œè¿”å›æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
        
        Returns:
            å‚æ•°åç§°åˆ°å‚æ•°å¼ é‡çš„æ˜ å°„
        """
        if self._lora_enabled and self.lora_wrapper is not None:
            # è·å–LoRAå‚æ•°
            lora_params = self.lora_wrapper.get_lora_parameters()
            
            # è·å–åˆ†ç±»å¤´å‚æ•°
            classifier_params = {
                f"classifier.{name}": param.data.clone()
                for name, param in self.classification_head.named_parameters()
                if param.requires_grad
            }
            
            # åˆå¹¶LoRAå‚æ•°å’Œåˆ†ç±»å¤´å‚æ•°
            all_params = {**lora_params, **classifier_params}
            return all_params
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šè¿”å›æ‰€æœ‰å¯è®­ç»ƒå‚æ•°
            return {
                name: param.data.clone()
                for name, param in self.classifier.named_parameters()
                if param.requires_grad
            }
    
    def set_parameters(self, params: Dict[str, torch.Tensor]):
        """
        è®¾ç½®æ¨¡å‹å‚æ•° - è”é‚¦å­¦ä¹ æ ¸å¿ƒåŠŸèƒ½
        
        Args:
            params: å‚æ•°åç§°åˆ°å‚æ•°å¼ é‡çš„æ˜ å°„
        """
        if self._lora_enabled and self.lora_wrapper is not None:
            # åˆ†ç¦»LoRAå‚æ•°å’Œåˆ†ç±»å¤´å‚æ•°
            lora_params = {}
            classifier_params = {}
            
            for name, param in params.items():
                if name.startswith("vision.") or name.startswith("text."):
                    lora_params[name] = param
                elif name.startswith("classifier."):
                    classifier_params[name[11:]] = param  # ç§»é™¤"classifier."å‰ç¼€
            
            # è®¾ç½®LoRAå‚æ•°
            if lora_params:
                self.lora_wrapper.set_lora_parameters(lora_params)
            
            # è®¾ç½®åˆ†ç±»å¤´å‚æ•°
            if classifier_params:
                with torch.no_grad():
                    for name, param in self.classification_head.named_parameters():
                        if name in classifier_params and param.requires_grad:
                            param.data.copy_(classifier_params[name])
        else:
            # æ ‡å‡†æ¨¡å¼ï¼šè®¾ç½®æ‰€æœ‰å‚æ•°
            with torch.no_grad():
                for name, param in self.classifier.named_parameters():
                    if name in params and param.requires_grad:
                        param.data.copy_(params[name])
    
    def _get_model_device(self):
        """è·å–æ¨¡å‹æ‰€åœ¨è®¾å¤‡ - å¸¦ç¼“å­˜ä¼˜åŒ–"""
        if self._device_cache is None or self._device_cache_dirty:
            if hasattr(self.classifier, 'parameters'):
                try:
                    self._device_cache = next(self.classifier.parameters()).device
                    self._device_cache_dirty = False
                except StopIteration:
                    self._device_cache = torch.device('cpu')
            elif hasattr(self.image_encoder, 'parameters'):
                try:
                    self._device_cache = next(self.image_encoder.parameters()).device
                    self._device_cache_dirty = False
                except StopIteration:
                    self._device_cache = torch.device('cpu')
            else:
                self._device_cache = torch.device('cpu')
        return self._device_cache
    
    def train_step(self, data: torch.Tensor, labels: torch.Tensor) -> float:
        """
        å•æ­¥è®­ç»ƒ
        
        Args:
            data: è¾“å…¥å›¾åƒæ•°æ®
            labels: æ ‡ç­¾
            
        Returns:
            è®­ç»ƒæŸå¤±
        """
        self.classifier.train()
        self.optimizer.zero_grad()
        
        # è·å–ç¼“å­˜çš„è®¾å¤‡ï¼Œé¿å…é‡å¤è°ƒç”¨
        device = self._get_model_device()
        data, labels = device_manager.move_tensors_to_device(data, labels, device=device)
        
        # å‰å‘ä¼ æ’­
        outputs = self.classifier(data)
        loss = self.criterion(outputs, labels)
        
        # åå‘ä¼ æ’­
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆå¯é€‰ï¼‰
        torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        return loss.item()
    
    def evaluate(self, data: torch.Tensor, labels: torch.Tensor) -> Dict[str, float]:
        """
        æ¨¡å‹è¯„ä¼°
        
        Args:
            data: è¯„ä¼°æ•°æ®
            labels: çœŸå®æ ‡ç­¾
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        self.classifier.eval()
        
        with torch.no_grad():
            outputs = self.classifier(data)
            loss = self.criterion(outputs, labels)
            
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs, 1)
            total = labels.size(0)
            correct = (predicted == labels).sum().item()
            accuracy = correct / total
            
            # è®¡ç®—Top-5å‡†ç¡®ç‡ï¼ˆå¦‚æœç±»åˆ«æ•°>=5ï¼‰
            top5_accuracy = None
            if self.num_classes >= 5:
                _, top5_pred = outputs.topk(5, 1, largest=True, sorted=True)
                top5_correct = top5_pred.eq(labels.view(-1, 1).expand_as(top5_pred)).sum().item()
                top5_accuracy = top5_correct / total
        
        result = {
            'loss': loss.item(),
            'accuracy': accuracy
        }
        
        if top5_accuracy is not None:
            result['top5_accuracy'] = top5_accuracy
            
        return result
    
    def predict(self, data: torch.Tensor) -> torch.Tensor:
        """
        é¢„æµ‹
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            é¢„æµ‹ç»“æœ
        """
        self.classifier.eval()
        with torch.no_grad():
            outputs = self.classifier(data)
            _, predicted = torch.max(outputs, 1)
        return predicted
    
    def predict_proba(self, data: torch.Tensor) -> torch.Tensor:
        """
        é¢„æµ‹æ¦‚ç‡
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            é¢„æµ‹æ¦‚ç‡
        """
        self.classifier.eval()
        with torch.no_grad():
            outputs = self.classifier(data)
            probabilities = F.softmax(outputs, dim=1)
        return probabilities
    
    def get_features(self, data: torch.Tensor) -> torch.Tensor:
        """
        æå–ç‰¹å¾
        
        Args:
            data: è¾“å…¥æ•°æ®
            
        Returns:
            ç‰¹å¾å‘é‡
        """
        self.classifier.eval()
        with torch.no_grad():
            features = self.image_encoder(data)
        return features
    
    def evaluate_with_dataloader(self, data_loader) -> Dict[str, float]:
        """
        ä½¿ç”¨æ•°æ®åŠ è½½å™¨è¯„ä¼°æ¨¡å‹
        
        Args:
            data_loader: æ•°æ®åŠ è½½å™¨
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        self.classifier.eval()
        
        total_loss = 0.0
        total_samples = 0
        correct_predictions = 0
        top5_correct = 0
        
        # è·å–ç¼“å­˜çš„è®¾å¤‡ï¼Œé¿å…æ¯æ¬¡batchéƒ½é‡å¤è°ƒç”¨
        device = self._get_model_device()
        
        with torch.no_grad():
            for batch_data, batch_labels in data_loader:
                # ç®€åŒ–è®¾å¤‡ç§»åŠ¨æ“ä½œ
                batch_data, batch_labels = device_manager.move_tensors_to_device(
                    batch_data, batch_labels, device=device
                )
                
                # å‰å‘ä¼ æ’­
                outputs = self.classifier(batch_data)
                loss = self.criterion(outputs, batch_labels)
                
                # ä¿®æ­£ï¼šä½¿ç”¨æ ·æœ¬æ•°åŠ æƒå¹³å‡
                total_loss += loss.item() * batch_data.size(0)
                total_samples += batch_data.size(0)
                
                # è®¡ç®—å‡†ç¡®ç‡
                _, predicted = torch.max(outputs, 1)
                correct_predictions += (predicted == batch_labels).sum().item()
                
                # è®¡ç®—Top-5å‡†ç¡®ç‡ï¼ˆå¦‚æœç±»åˆ«æ•°>=5ï¼‰
                if self.num_classes >= 5:
                    _, top5_pred = outputs.topk(5, 1, largest=True, sorted=True)
                    top5_correct += top5_pred.eq(batch_labels.view(-1, 1).expand_as(top5_pred)).sum().item()
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡ï¼Œä½¿ç”¨æ ·æœ¬æ•°åŠ æƒå¹³å‡
        avg_loss = total_loss / total_samples if total_samples > 0 else 0.0
        accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0
        
        result = {
            'loss': avg_loss,
            'accuracy': accuracy
        }
        
        # æ·»åŠ Top-5å‡†ç¡®ç‡ï¼ˆå¦‚æœé€‚ç”¨ï¼‰
        if self.num_classes >= 5:
            top5_accuracy = top5_correct / total_samples if total_samples > 0 else 0.0
            result['top5_accuracy'] = top5_accuracy
            
        return result
    
    def save_model(self, filepath: str):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            filepath: ä¿å­˜è·¯å¾„
        """
        torch.save({
            'classifier_state_dict': self.classifier.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'model_config': {
                'model_name': self.model_name,
                'num_classes': self.num_classes,
                'normalize_features': self.normalize_features,
                'cache_dir': self.cache_dir
            }
        }, filepath)
        print(f"CLIP model saved to {filepath}")
    
    def load_model(self, filepath: str):
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            filepath: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        """
        checkpoint = torch.load(filepath, map_location='cpu')
        self.classifier.load_state_dict(checkpoint['classifier_state_dict'])
        if 'optimizer_state_dict' in checkpoint:
            try:
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                print("Optimizer state loaded")
            except Exception as e:
                print(f"Warning: Could not load optimizer state: {e}")
        print(f"CLIP model loaded from {filepath}")
    
    @classmethod
    def from_checkpoint(cls, checkpoint_path: str, **kwargs):
        """
        ä»checkpointæ–‡ä»¶åˆ›å»ºCLIPæ¨¡å‹çš„ç±»æ–¹æ³•
        
        Args:
            checkpoint_path: checkpointæ–‡ä»¶è·¯å¾„
            **kwargs: é¢å¤–çš„åˆå§‹åŒ–å‚æ•°ï¼Œå°†è¦†ç›–checkpointä¸­çš„é…ç½®
            
        Returns:
            ä»checkpointåŠ è½½çš„CLIPæ¨¡å‹å®ä¾‹
        """
        print(f"Creating CLIP model from checkpoint: {checkpoint_path}")
        
        # åŠ è½½checkpointè·å–é…ç½®
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        config = checkpoint.get('model_config', {})
        
        # åˆå¹¶checkpointé…ç½®å’Œä¼ å…¥çš„å‚æ•°ï¼Œä¼ å…¥çš„å‚æ•°å…·æœ‰æ›´é«˜ä¼˜å…ˆçº§
        init_kwargs = {
            'model_name': config.get('model_name', 'openai/clip-vit-base-patch32'),
            'num_classes': config.get('num_classes', 10),
            'normalize_features': config.get('normalize_features', True),
            'cache_dir': config.get('cache_dir', None),
            'checkpoint_path': checkpoint_path  # è‡ªåŠ¨åŠ è½½æƒé‡
        }
        
        # ç”¨ä¼ å…¥çš„å‚æ•°è¦†ç›–checkpointé…ç½®
        init_kwargs.update(kwargs)
        
        return cls(**init_kwargs)
    
    def get_model_summary(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹æ‘˜è¦"""
        total_params = sum(p.numel() for p in self.classifier.parameters())
        trainable_params = sum(p.numel() for p in self.classifier.parameters() if p.requires_grad)
        
        summary = {
            'model_name': self.model_name,
            'num_classes': self.num_classes,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'encoder_feature_dim': self.image_encoder.feature_dim,
            'normalize_features': self.normalize_features,
            'lora_enabled': self._lora_enabled
        }
        
        # æ·»åŠ LoRAç‰¹å®šä¿¡æ¯
        if self._lora_enabled and self.lora_wrapper is not None:
            lora_trainable_params = self.lora_wrapper.get_trainable_parameters()
            summary.update({
                'lora_trainable_parameters': lora_trainable_params,
                'lora_status': self.lora_wrapper.is_lora_applied()
            })
        
        return summary
    
    def is_lora_enabled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†LoRA"""
        return self._lora_enabled
    
    def get_lora_info(self) -> Dict[str, Any]:
        """è·å–LoRAç›¸å…³ä¿¡æ¯"""
        if not self._lora_enabled or self.lora_wrapper is None:
            return {'enabled': False}
        
        return {
            'enabled': True,
            'status': self.lora_wrapper.is_lora_applied(),
            'trainable_parameters': self.lora_wrapper.get_trainable_parameters(),
            'config': self.lora_config
        }


# ç»Ÿä¸€çš„å·¥å‚å‡½æ•°ï¼Œæ”¯æŒä»é…ç½®æˆ–checkpointåˆ›å»ºCLIPæ¨¡å‹
def create_clip_model(config: Dict[str, Any]) -> FederatedCLIPModel:
    """
    åˆ›å»ºè”é‚¦å­¦ä¹ CLIPæ¨¡å‹çš„ç»Ÿä¸€å·¥å‚å‡½æ•°
    
    Args:
        config: æ¨¡å‹é…ç½®å­—å…¸ï¼Œå¯ä»¥åŒ…å«ä»¥ä¸‹é”®ï¼š
            - model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°
            - num_classes: åˆ†ç±»ç±»åˆ«æ•°
            - normalize_features: æ˜¯å¦å¯¹ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–
            - freeze_encoder: æ˜¯å¦å†»ç»“ç¼–ç å™¨
            - cache_dir: æ¨¡å‹ç¼“å­˜ç›®å½•
            - optimizer_config: ä¼˜åŒ–å™¨é…ç½®
            - checkpoint_path: å¦‚æœæä¾›ï¼Œå°†ä»æ­¤è·¯å¾„åŠ è½½æ¨¡å‹æƒé‡
            - lora: LoRAé…ç½®å­—å…¸
        
    Returns:
        è”é‚¦å­¦ä¹ CLIPæ¨¡å‹å®ä¾‹
    """
    # å¦‚æœæä¾›äº†checkpointè·¯å¾„ï¼Œä¼˜å…ˆä½¿ç”¨from_checkpointæ–¹æ³•
    if 'checkpoint_path' in config and config['checkpoint_path'] is not None:
        checkpoint_path = config.pop('checkpoint_path')
        return FederatedCLIPModel.from_checkpoint(checkpoint_path, **config)
    
    # å¦åˆ™ç›´æ¥åˆ›å»ºæ–°æ¨¡å‹
    return FederatedCLIPModel(
        model_name=config.get('model_name', 'openai/clip-vit-base-patch32'),
        num_classes=config.get('num_classes', 10),
        normalize_features=config.get('normalize_features', True),
        freeze_encoder=config.get('freeze_encoder', False),
        cache_dir=config.get('cache_dir', None),
        optimizer_config=config.get('optimizer_config', None),
        lora_config=config.get('lora', None)
    )
