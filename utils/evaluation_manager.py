"""
ç»Ÿä¸€è¯„ä¼°ç®¡ç†å™¨
è´Ÿè´£è”é‚¦å­¦ä¹ ä¸­å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨çš„è¯„ä¼°é€»è¾‘
"""

from typing import Dict, Any, List, Optional
from dataclasses import dataclass


@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœæ•°æ®ç±»"""
    round_num: int
    client_metrics: Dict[str, Dict[str, float]]  # {client_id: {metric_name: value}}
    global_metrics: Dict[str, float]  # {metric_name: value}
    dataset_sample_counts: Optional[Dict[str, int]] = None


class EvaluationManager:
    """
    ç»Ÿä¸€è¯„ä¼°ç®¡ç†å™¨
    
    è´Ÿè´£ï¼š
    1. å®¢æˆ·ç«¯æœ¬åœ°è¯„ä¼°
    2. å…¨å±€æ¨¡å‹è¯„ä¼°  
    3. è¯„ä¼°ç»“æœæ ¼å¼åŒ–
    4. è¯„ä¼°è¾“å‡ºç®¡ç†
    """
    
    def __init__(self, verbose: bool = True):
        """
        åˆå§‹åŒ–è¯„ä¼°ç®¡ç†å™¨
        
        Args:
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        self.verbose = verbose
        self.evaluation_history: List[EvaluationResult] = []
    
    def evaluate_clients(self, clients: List, round_num: int) -> Dict[str, Dict[str, float]]:
        """
        è¯„ä¼°æ‰€æœ‰å®¢æˆ·ç«¯çš„æœ¬åœ°æ¨¡å‹
        
        Args:
            clients: å®¢æˆ·ç«¯åˆ—è¡¨
            round_num: å½“å‰è½®æ¬¡
            
        Returns:
            å®¢æˆ·ç«¯è¯„ä¼°ç»“æœå­—å…¸
        """
        client_metrics = {}
        
        if self.verbose:
            print(f"  ğŸ“± è¯„ä¼°å®¢æˆ·ç«¯æœ¬åœ°æ¨¡å‹...")
            
        for client in clients:
            try:
                metrics = client.evaluate_on_local_data()
                if metrics:
                    client_metrics[client.client_id] = metrics
                    if self.verbose:
                        acc = metrics.get('accuracy', 0) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
                        loss = metrics.get('loss', 0)
                        print(f"    {client.client_id}: å‡†ç¡®ç‡ {acc:.2f}%, æŸå¤± {loss:.4f}")
                        
            except Exception as e:
                if self.verbose:
                    print(f"    âš ï¸  {client.client_id} è¯„ä¼°å¤±è´¥: {str(e)}")
                    
        return client_metrics
    
    def evaluate_global_model(self, server, test_loaders: Dict, round_num: int) -> Dict[str, float]:
        """
        è¯„ä¼°å…¨å±€æ¨¡å‹
        
        Args:
            server: è”é‚¦å­¦ä¹ æœåŠ¡å™¨
            test_loaders: æµ‹è¯•æ•°æ®åŠ è½½å™¨å­—å…¸
            round_num: å½“å‰è½®æ¬¡
            
        Returns:
            å…¨å±€æ¨¡å‹è¯„ä¼°ç»“æœå­—å…¸
        """
        if not test_loaders:
            if self.verbose:
                print("    âš ï¸  æ— æµ‹è¯•æ•°æ®é›†ï¼Œè·³è¿‡å…¨å±€è¯„ä¼°")
            return {}
        
        if self.verbose:
            print(f"  ğŸŒ è¯„ä¼°å…¨å±€æ¨¡å‹...")
            
        all_metrics = {}
        dataset_sample_counts = {}
        failed_count = 0
        
        # è¯„ä¼°æ¯ä¸ªæ•°æ®é›†
        for dataset_name, test_loader in test_loaders.items():
            try:
                dataset_metrics = server.evaluate_with_dataloader(test_loader)
                if dataset_metrics:
                    dataset_sample_counts[dataset_name] = len(test_loader.dataset)
                    
                    # æ·»åŠ æ•°æ®é›†å‰ç¼€
                    for metric_name, value in dataset_metrics.items():
                        all_metrics[f"{dataset_name}_{metric_name}"] = value
                        
                    if self.verbose:
                        acc = dataset_metrics.get('accuracy', 0) * 100
                        loss = dataset_metrics.get('loss', 0)
                        print(f"    {dataset_name}: å‡†ç¡®ç‡ {acc:.2f}%, æŸå¤± {loss:.4f}")
                        
            except Exception as e:
                failed_count += 1
                if self.verbose:
                    print(f"    âš ï¸  {dataset_name} è¯„ä¼°å¤±è´¥: {str(e)}")
        
        # è®¡ç®—åŠ æƒå¹³å‡æŒ‡æ ‡
        if len(dataset_sample_counts) > 0:
            avg_metrics = self._calculate_weighted_average(all_metrics, dataset_sample_counts)
            all_metrics.update(avg_metrics)
            
            if self.verbose and avg_metrics:
                avg_acc = avg_metrics.get('avg_accuracy', 0) * 100
                avg_loss = avg_metrics.get('avg_loss', 0)
                print(f"    ğŸ“Š å¹³å‡: å‡†ç¡®ç‡ {avg_acc:.2f}%, æŸå¤± {avg_loss:.4f}")
        
        if failed_count > 0 and self.verbose:
            print(f"    âš ï¸  {failed_count} ä¸ªæ•°æ®é›†è¯„ä¼°å¤±è´¥")
            
        return all_metrics
    
    def _calculate_weighted_average(self, all_metrics: Dict[str, float], 
                                  dataset_sample_counts: Dict[str, int]) -> Dict[str, float]:
        """è®¡ç®—åŠ æƒå¹³å‡æŒ‡æ ‡"""
        avg_metrics = {}
        
        # æå–æŒ‡æ ‡åç§°
        metric_names = set()
        for key in all_metrics.keys():
            for dataset_name in dataset_sample_counts.keys():
                if key.startswith(f"{dataset_name}_"):
                    metric_name = key[len(f"{dataset_name}_"):]
                    metric_names.add(metric_name)
                    break
        
        # è®¡ç®—åŠ æƒå¹³å‡
        total_samples = sum(dataset_sample_counts.values())
        
        for metric_name in metric_names:
            weighted_sum = 0.0
            valid_samples = 0
            
            for dataset_name, sample_count in dataset_sample_counts.items():
                metric_key = f"{dataset_name}_{metric_name}"
                if metric_key in all_metrics:
                    weighted_sum += all_metrics[metric_key] * sample_count
                    valid_samples += sample_count
            
            if valid_samples > 0:
                avg_metrics[f"avg_{metric_name}"] = weighted_sum / valid_samples
                
        return avg_metrics
    
    def create_evaluation_result(self, round_num: int, client_metrics: Dict[str, Dict[str, float]], 
                               global_metrics: Dict[str, float]) -> EvaluationResult:
        """åˆ›å»ºè¯„ä¼°ç»“æœå¯¹è±¡"""
        result = EvaluationResult(
            round_num=round_num,
            client_metrics=client_metrics,
            global_metrics=global_metrics
        )
        self.evaluation_history.append(result)
        return result
    
    def format_round_summary(self, result: EvaluationResult) -> str:
        """æ ¼å¼åŒ–è½®æ¬¡æ€»ç»“"""
        round_num = result.round_num
        global_metrics = result.global_metrics
        
        # ä¼˜å…ˆä½¿ç”¨å¹³å‡æŒ‡æ ‡
        accuracy = global_metrics.get('avg_accuracy', 0)
        loss = global_metrics.get('avg_loss', 0)
        
        # å¦‚æœæ²¡æœ‰å¹³å‡æŒ‡æ ‡ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨çš„æŒ‡æ ‡
        if accuracy == 0 and loss == 0:
            for key, value in global_metrics.items():
                if 'accuracy' in key and accuracy == 0:
                    accuracy = value
                elif 'loss' in key and loss == 0:
                    loss = value
        
        # è½¬æ¢å‡†ç¡®ç‡ä¸ºç™¾åˆ†æ¯”
        accuracy_pct = accuracy * 100 if accuracy <= 1.0 else accuracy
        
        return f"è½®æ¬¡ {round_num:2d}: å…¨å±€å‡†ç¡®ç‡ {accuracy_pct:.2f}%, å…¨å±€æŸå¤± {loss:.4f}"
    
    def get_final_summary(self) -> Dict[str, Any]:
        """è·å–æœ€ç»ˆå®éªŒæ€»ç»“"""
        if not self.evaluation_history:
            return {}
            
        final_result = self.evaluation_history[-1]
        first_result = self.evaluation_history[0]
        
        # è·å–æœ€ç»ˆå’Œåˆå§‹æŒ‡æ ‡
        final_global = final_result.global_metrics
        first_global = first_result.global_metrics
        
        # è®¡ç®—æ”¹è¿›æƒ…å†µ
        final_acc = final_global.get('avg_accuracy', 0)
        final_loss = final_global.get('avg_loss', 0)
        first_acc = first_global.get('avg_accuracy', 0)  
        first_loss = first_global.get('avg_loss', 0)
        
        # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
        final_acc_pct = final_acc * 100 if final_acc <= 1.0 else final_acc
        first_acc_pct = first_acc * 100 if first_acc <= 1.0 else first_acc
        
        return {
            'total_rounds': len(self.evaluation_history),
            'final_accuracy': final_acc_pct,
            'final_loss': final_loss,
            'accuracy_improvement': final_acc_pct - first_acc_pct,
            'loss_reduction': first_loss - final_loss,
            'client_count': len(final_result.client_metrics)
        }
